# 2021: A Year Full of Amazing AI papers- A Review<br/>  *[work in progress...]*
## A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, andcode.

While the world is still recovering, research hasn't slowed its frenetic pace, especially in the field of artificial intelligence. More, many important aspects were highlighted this year, like the ethical aspects, important biases, governance, transparency and much more. Artificial intelligence and our understanding of the human brain and its link to AI are constantly evolving, showing promising applications improving our life's quality in the near future. Still, we ought to be careful with which technology we choose to apply.

>"Science cannot tell us what we ought to do, only what we can do."<br/>- Jean-Paul Sartre, Being and Nothingness

Here are the most interesting research papers of the year, in case you missed any of them. In short, it is curated list of the latest breakthroughs in AI and Data Science by release date with a clear video explanation, link to a more in-depth article, and code (if applicable). Enjoy the read!

**The complete reference to each paper is listed at the end of this repository.**<br/>
*This is a work in progress... Star this repository to stay up to date!* 猸锔

Maintainer: [louisfb01](https://github.com/louisfb01)

Subscribe to my [newsletter](http://eepurl.com/huGLT5) - The latest updates in AI explained every week.


*Feel free to [message me](https://www.louisbouchard.ai/contact/) any interesting paper I may have missed to add to this repository.*

*Tag me on **Twitter** [@Whats_AI](https://twitter.com/Whats_AI) or **LinkedIn** [@Louis (What's AI) Bouchard](https://www.linkedin.com/in/whats-ai/) if you share the list!*

<!-- ### Watch a complete 2020 rewind in 15minutes

[![Watch the video](https://imgur.com/xzZT1ll.png)](https://youtu.be/DHBclF-8KwE)

--- -->

Missed last year? Check this out: [2020: A Year Full of Amazing AI papers- A Review](https://github.com/louisfb01/Best_AI_paper_2020)


## The Full List
- [DALL路E: Zero-Shot Text-to-Image Generation from OpenAI [1]](#1)
- [VOGUE: Try-On by StyleGAN Interpolation Optimization[2]](#2)
- [Taming Transformers for High-Resolution Image Synthesis[3]](#3)
- [todo[4]](#4)
- [todo [5]](#5)
- [todo [6]](#6)
- [todo[7]](#7)
- [todo [8]](#8)
- [todo[9]](#9)
- [todo [10]](#10)
- [todo[11]](#11)
- [todo [12]](#12)
- [todo[13]](#13)
- [todo[14]](#14)
- [todo[15]](#15)
- [todo [16]](#16)
- [todo[17]](#17)
- [todo[18]](#18)
- [todo [19]](#19)
- [todo[20]](#20)
- [todo [21]](#21)
- [todo [22]](#22)
- [todo[23]](#23)
- [todo[24]](#24)
- [todo[25]](#25)
- [todo[26]](#26)
- [todo [27]](#27)
- [todo [28]](#28)
- [Paper references](#references)

---

## DALL路E: Zero-Shot Text-to-Image Generation from OpenAI [1]<a name="1"></a>
OpenAI successfully trained a network able to generate images from text captions. It is very similar to GPT-3 and Image GPT and produces amazing results.

* Short Video Explanation:

[![Watch the video](https://imgur.com/Czdyuce.png)](https://www.youtube.com/DJToDLBPovg)
* [OpenAIs DALL路E: Text-to-Image Generation Explained](https://www.louisbouchard.ai/openais-dall-e-text-to-image-generation-explained/) - Short Read
* [Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092.pdf) - The Paper
* [Code & more information for the discrete VAE used for DALL路E](https://github.com/openai/DALL-E) - The Code


## VOGUE: Try-On by StyleGAN Interpolation Optimization[2]<a name="2"></a>
Google used a modified StyleGAN2 architecture to create an online fitting room where you can automatically try-on any pants or shirts you want using only an image of yourself.

* Short Video Explanation:

[![Watch the video](https://imgur.com/FQL9bwU.png)](https://youtu.be/i4MnLJGZbaM)
* [The AI-Powered Online Fitting Room: VOGUE](https://medium.com/towards-artificial-intelligence/the-ai-powered-online-fitting-room-vogue-5f77c599832) - Short Read
* [VOGUE: Try-On by StyleGAN Interpolation Optimization](https://vogue-try-on.github.io/static_files/resources/VOGUE-virtual-try-on.pdf) - The Paper


## Taming Transformers for High-Resolution Image Synthesis[3]<a name="3"></a>
Tl;DR: They combined the efficiency of GANs and convolutional approaches with the expressivity of transformers to produce a powerful and time-efficient method for semantically-guided high-quality image synthesis.

* Short Video Explanation:

[![Watch the video](https://imgur.com/0zUY1tm.png)](https://youtu.be/JfUTd8fjtX8)
* [Combining the Transformers Expressivity with the CNNs Efficiency for High-Resolution Image Synthesis](https://medium.com/towards-artificial-intelligence/combining-the-transformers-expressivity-with-the-cnns-efficiency-for-high-resolution-image-synthesis-31c6767547da) - Short Read
* [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2005.12126.pdf) - The Paper
* [Taming Transformers](https://github.com/CompVis/taming-transformers) - The Code


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


## title_goes_here[4]<a name="4"></a>
my_text_goes_here

* Short Video Explanation:

[![Watch the video](imgur_link.png)](youtube_link)
* [article_title](article_link) - Short Read
* [paper_title](https://arxiv.org/abs/2003.03808) - The Paper
* [Click here for the code](github_link)


---


### If you would like to read more papers and have a broader view, here is another great repository for you covering 2020:
[2020: A Year Full of Amazing AI papers- A Review](https://github.com/louisfb01/Best_AI_paper_2020)


*Tag me on **Twitter** [@Whats_AI](https://twitter.com/Whats_AI) or **LinkedIn** [@Louis (What's AI) Bouchard](https://www.linkedin.com/in/whats-ai/) if you share the list!*

---

## Paper references<a name="references"></a>

[1] A. Ramesh et al., Zero-shot text-to-image generation, 2021. arXiv:2102.12092

[2] Lewis, Kathleen M et al., (2021), VOGUE: Try-On by StyleGAN Interpolation Optimization.

[3] Taming Transformers for High-Resolution Image Synthesis, Esser et al., 2020.

[4] todo

[5] todo

[6] todo

[7] todo

[8] todo

[9] todo

[10] todo

[11] todo

[12] todo

[13] todo

[14] todo

[15] todo

[16] todo

[17] todo

[18] todo

[19] todo

[20] todo

[21] todo

[22] todo

[23] todo

[24] todo

[25] todo

[26] todo

[27] todo

[28] todo



